model:
  internvl_path: /data/phd/jinjiachun/ckpt/OpenGVLab/InternVL3_5-1B
  down_proj:
    input_dim: 4096
    output_dim: 16
    hidden_size: 1024
    depth: 16
    num_heads: 16
    grid_size: 16
    llm_hidden_size: 1024
  tune_llm: True
  # 词汇表扩展配置
  vocab_expansion:
    enabled: true
    embedding_init_method: "random"  # "random", "zeros", "mean"
    embedding_init_std: 0.02
    freeze_original: true  # 冻结原始embeddings，只训练新扩展的部分

train:
  root: /data/phd/jinjiachun/experiment
  resume_path: /data/phd/jinjiachun/experiment/vq_llava_distill/0923_llava_lfq_transformer_debug/internvl-vq_llava_distill-32000
  global_step: 0

  exp_name: &exp_name vq_llava_distill
  proj_name: *exp_name
  output_dir: 0924_joint_lfq
  logging_dir: logs
  mixed_precision: bf16
  gradient_accumulation_steps: 1
  report_to: null

  lr: 1e-4
  num_iter: 100000
  save_every: 2000

  hp_loss_gen: 1.0
  hp_loss_und: 1.0

data:
  batch_size: &batch_size 24
  und:
    batch_size: 6
    num_workers: 2
    max_seq_length: 512
  gen:
    wds_path: [/data/phd/jinjiachun/dataset/BLIP3o/BLIP3o-Pretrain-Long-Caption, /data/phd/jinjiachun/dataset/BLIP3o/BLIP3o-Pretrain-Short-Caption, /data/phd/jinjiachun/dataset/BLIP3o/BLIP3o-Pretrain-JourneyDB]
    img_size: 448
    cfg_drop_rate: 0.1
    num_img_token: 256
    max_seq_length: 512
    buffer_size: 100
    batch_size: *batch_size
    num_workers: 8
